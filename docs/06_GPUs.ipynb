{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8eda9a1-0b12-40da-836e-96f0cebf1b25",
   "metadata": {},
   "source": [
    "# GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c16cb6-0cba-4fda-acf4-03b9a791df9a",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukeconibear/swd6_hpp/blob/main/docs/07_GPUs.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb8a2fac-5de3-4a2b-8db7-f061fdafd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c417818-9ff3-42f3-8e45-453b9a8e1c4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "GPUs (Graphics Processing Units) are optimised for numerical operations, while CPUs (central processing units) perform general computation.\n",
    "\n",
    "GPU hardware is designed for data parallelism, where high throughputs are achieved when the GPU is computing the same operations on many different elements at once.\n",
    "\n",
    "You could use other types of accelerators too, though we're not going to cover those here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01cc79-c522-4fcf-8aae-9ab3790ddfdb",
   "metadata": {},
   "source": [
    "## [Numba for CUDA GPUs](http://numba.pydata.org/numba-doc/latest/cuda/index.html)\n",
    "\n",
    "Earlier we covered how Numba works on single CPUs with [`@njit`](https://numba.readthedocs.io/en/stable/glossary.html#term-nopython-mode) and multiple CPUs with `parallel = True`.\n",
    "\n",
    "As a recap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8e522ad-e488-4eec-a720-bcd745f5dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "772aac51-cebd-45ae-b5a9-3fa688b7e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1.e7, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5963d67-56fa-4cf5-bd7b-ad8917e08972",
   "metadata": {},
   "source": [
    "So, for a single CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d193f96-15b4-4872-8a80-81aa8803bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def my_serial_function_for_cpu(x):\n",
    "    return np.cos(x) ** 2 + np.sin(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12f6fc56-d9ee-48d7-9187-3f53f68a0a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 ms ± 1.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "my_serial_function_for_cpu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54839928-bd48-4501-ba46-4baff7d998fc",
   "metadata": {},
   "source": [
    "And, for multiple CPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bb1bc75-2961-4f4b-9442-b2329424be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def my_parallel_function_for_cpu(x):\n",
    "    return np.cos(x) ** 2 + np.sin(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e45b0a3c-0fa4-4ceb-818b-bcea23df7538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.4 ms ± 961 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "my_parallel_function_for_cpu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b62ba-1689-419e-a3d3-aae3b72621b4",
   "metadata": {},
   "source": [
    "*Note, here we used `njit` as this automates the parallelisation process.*\n",
    "\n",
    "*This is in contrast to `vectorize` where manual effort is required for parallelisation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040c473-ea57-449b-a6e9-9632ab0506fa",
   "metadata": {},
   "source": [
    "```{note} \n",
    "If you're in COLAB or have a local CUDA GPU, you can follow along with this section (uncomment the code).\n",
    "\n",
    "For those in COLAB, ensure the session is using a GPU by going to: Runtime > Change runtime type > Hardware accelerator = GPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c35a4-ffbf-4a46-8915-454a777d0ac5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `vectorize` for GPUs\n",
    "\n",
    "Numba also works on [CUDA](https://developer.nvidia.com/how-to-cuda-python) GPUs using [`@vectorize`](https://numba.pydata.org/numba-doc/latest/user/vectorize.html) or [`@cuda.jit`](https://numba.readthedocs.io/en/stable/cuda/kernels.html).\n",
    "\n",
    "This is suitable for bigger data sizes (> 1 MB) and high compute intensities.\n",
    "\n",
    "This adds additional overhead due to moving data to and from GPUs ([memory management](https://numba.pydata.org/numba-doc/dev/cuda/memory.html)).\n",
    "\n",
    "Similar to our examples in the compiler lesson, we need to specify the types and target in the signature (i.e., the decorator arguments).\n",
    "\n",
    "Here, the types are specificed slightly differently i.e. output types(input types).\n",
    "\n",
    "*Note, not all NumPy code will work on the GPU ([supported](https://numba.pydata.org/numba-doc/latest/reference/numpysupported.html)). In the following example, we will need to use the `math` library instead.*\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "52821ef6-0aec-4611-8160-1d12dcf43f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numba import vectorize, float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6016dc81-e787-4d1c-8b8e-0d415571aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1.e7, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab6ca02f-c528-49b1-8bba-af3c45989406",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def my_serial_function_for_gpu(x):\n",
    "    return math.cos(x) ** 2 + math.sin(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "28ccf4a9-77c8-41ac-99c3-fcef0bf3d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# my_serial_function_for_gpu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f99144-e853-4ad6-89b2-7c75f7339113",
   "metadata": {},
   "source": [
    "Numba also supports generalized ufuncs (covered in the compiler lesson) on the GPU using [`guvectorize`](http://numba.pydata.org/numba-doc/latest/cuda/ufunc.html#generalized-cuda-ufuncs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6466145b-ae8a-48fc-bfe0-f41e47e117b9",
   "metadata": {},
   "source": [
    "### Custom CUDA kernels\n",
    "\n",
    "Kernel functions are GPU functions called from CPU code.\n",
    "\n",
    "Kernels cannot explicitly return a value. Instead, all result data must be written to an array passed to the function (e.g., called `out`). This array can then be transferred back.\n",
    "\n",
    "Kernels work over a grid of threads. This grid needs to be defined in terms of the number of blocks in the grid and the number of threads per block. The indices of this grid are used to add values to the `out` array. The indices can be found using `cuda.grid(1)` for a 1D grid.\n",
    "\n",
    "CUDA kernels are compiled using the `numba.cuda.jit` decorator.\n",
    "\n",
    "*Note, `numba.cuda.jit` is different to `numba.jit`, which is for CPUs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f0f44a6-644d-4be2-bcb6-b4a7e0269122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "abd244d7-130f-44f1-9b0e-0636e715dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1aaac-a458-478f-a986-9a66eed89d47",
   "metadata": {},
   "source": [
    "This should return a message similar to: <Managed Device 0>.\n",
    "\n",
    "You can also run the bash command `nvidia-smi` within the IPython cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "359a0c10-157c-41eb-89d6-8e8ef870014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6250f7-46ea-42b8-9240-b41fd3b7f771",
   "metadata": {},
   "source": [
    "This returns something like the table below. This shows we have access to a [NVIDIA Tesla T4 GPU](https://www.nvidia.com/en-gb/data-center/tesla-t4/).\n",
    "\n",
    "```bash\n",
    "Tue Feb 22 13:59:03 2022       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
    "| N/A   66C    P0    30W /  70W |    144MiB / 15109MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ccf896-c980-4923-a750-7c01646afbdd",
   "metadata": {},
   "source": [
    "So, a simple example to add two numbers together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7c75ced9-32a4-44fd-a0a1-b33f4ff9dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @cuda.jit\n",
    "# def add_kernel(x, y, out):      \n",
    "#     index = cuda.grid(1)\n",
    "#     out[index] = x[index] + y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d49dbb-228c-403a-a91c-c0a51af88e3e",
   "metadata": {},
   "source": [
    "Let's define some input variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca0002ff-4460-4bfb-8c29-cb578e0d263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 4096\n",
    "# x = np.arange(n).astype(np.int32) # [0...4095] on the host\n",
    "# y = np.ones_like(x)               # [1...1] on the host"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56400925-e246-4d6f-af03-bcd734b64f6e",
   "metadata": {},
   "source": [
    "Now, let's move these input variables from the host (CPU) to the device (GPU) for the work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "661bbb47-08ab-4836-acd7-f1dad5d94f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_on_device = cuda.to_device(x)\n",
    "# y_on_device = cuda.to_device(y)\n",
    "# out_on_device = cuda.device_array_like(x_on_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795dad6c-0f97-4594-afdf-de4fee6d0448",
   "metadata": {},
   "source": [
    "Now, we [choose the block size](https://numba.pydata.org/numba-doc/latest/cuda/kernels.html#choosing-the-block-size), by defining how many blocks are in the grid and how many threads are in each of those blocks.\n",
    "\n",
    "These two numbers multipled together is the size of the grid (for our 1D example).\n",
    "\n",
    "Some rules of thumb are:\n",
    "- Blocks per grid should be a multiple of 32.\n",
    "- Threads per block should be a multiple of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dec6b274-e053-42f8-9b37-36f10516c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blocks_per_grid = 32\n",
    "# threads_per_block = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a03f0f-a0db-4502-834c-0174258b5774",
   "metadata": {},
   "source": [
    "Now, we can call the kernel function.\n",
    "\n",
    "First, add the grid size arguments.\n",
    "\n",
    "Then, we pass the input/output variables as arguments to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d3b8f3d-7c39-4ada-b553-5dd93a925afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_kernel[blocks_per_grid, threads_per_block](x_on_device, y_on_device, out_on_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd73a6-161f-49f9-b5a3-698c4d313527",
   "metadata": {},
   "source": [
    "As these CUDA kernels don't return a value, we can synchronise the device (GPU) back to the host (CPU) to get the result back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544215f1-b446-4a68-a41f-1d41c6d2964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda.synchronize()\n",
    "# print(out_on_device.copy_to_host())\n",
    "# # Should be [   1    2    3 ... 4094 4095 4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e915bd-1abd-4f87-9437-5e51abb2625d",
   "metadata": {},
   "source": [
    "For more information on CUDA, see the training courses:\n",
    "\n",
    "- [HPC5: Introduction to GPU programming with CUDA](https://arc.leeds.ac.uk/training/courses/hpc5/)\n",
    "- NVIDIA workshop on [Fundamentals of Accelerated Computing with CUDA Python](https://www.nvidia.com/en-us/training/instructor-led-workshops/fundamentals-of-accelerated-computing-with-cuda-python/)\n",
    "    - Detailed look at [custom CUDA kernels](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) and [GPU memory management](https://numba.pydata.org/numba-doc/dev/cuda/memory.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b679d8-1df4-4cbb-a7ad-161e8ea4658d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [RAPIDS](https://developer.nvidia.com/rapids)\n",
    "\n",
    "RAPIDS is a range of accelerated data science libraries from NVIDIA.\n",
    "\n",
    "There are a wide variety of tools matching up to familiar libraries:\n",
    "\n",
    "- Arrays and matrices\n",
    "  - [cuPy](https://cupy.dev/) for NumPy and SciPy\n",
    "- Tabular data\n",
    "    - [cuDF](https://docs.rapids.ai/api/cudf/stable/) for Pandas\n",
    "- Machine learning\n",
    "    - [cuML](https://docs.rapids.ai/api/cuml/stable/) for scikit-learn\n",
    "    - [XGBoost](https://rapids.ai/xgboost.html) on GPUs\n",
    "- Graphs and networks\n",
    "    - [cuGraph](https://docs.rapids.ai/api/cugraph/stable/) for [NetworkX](https://networkx.org/)\n",
    "- Multiple GPUs\n",
    "    - [Dask with CUDA](https://rapids.ai/dask.html), cuDF, cuML, and others.\n",
    "    - [Dask-MPI with GPUs](http://mpi.dask.org/en/latest/gpu.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52016c4b-e769-4a0d-ae9a-a223bc352f89",
   "metadata": {},
   "source": [
    "### [cuPy](https://cupy.dev/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a57d09-8e88-4485-a136-e799f053b750",
   "metadata": {},
   "source": [
    "**NumPy for the CPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cfce7-9ceb-442c-94e7-683f0abc8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a749fe7-a4e7-417c-a25f-abea5e6075ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.random.rand(1_000, 1_000)\n",
    "y_cpu = np.random.rand(1_000, 1_000)\n",
    "z_cpu = np.dot(x_cpu, y_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff284f-ec8b-4774-851a-1941325c3a67",
   "metadata": {},
   "source": [
    "**CuPy for the GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "16d343d4-95d8-4007-9258-25c9f119f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c055b298-a374-48a9-8d99-0b1c1be10117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_gpu = cp.random.rand(1_000, 1_000)\n",
    "# y_gpu = cp.random.rand(1_000, 1_000)\n",
    "# z_gpu = cp.dot(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5185093-82f3-4af2-b5d8-1d702ba81c0a",
   "metadata": {},
   "source": [
    "You can move arrays between the CPU and GPU as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dc6c08dd-618d-4e37-8249-232cfafbf861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_cpu = cp.asnumpy(z_gpu)  # from gpu to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "52f1237b-5dcf-4094-b52c-2cbbc6a5a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_gpu = cp.asarray(z_cpu)  # from cpu to gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e63d3f-4113-469d-b895-b36319cb4843",
   "metadata": {},
   "source": [
    "For more information on RAPIDS, see the training courses:\n",
    "\n",
    "- NVIDIA workshop on [Fundamentals of Accelerated Data Science (RAPIDS)](https://www.nvidia.com/en-us/training/instructor-led-workshops/fundamentals-of-accelerated-data-science/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68fa212-6b88-467b-ae53-75c5f64109f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "Similar to the Dask Dashboard, NVIDIA has a GPU Dashboard called [`NVDashboard`](https://github.com/rapidsai/jupyterlab-nvdashboard).\n",
    "\n",
    "These real-time diagnostics are provided via a Bokeh server and a Jupyter Lab extension.  \n",
    "\n",
    "They are a great way to manage your GPU utilisation, resources, throughput, and more.  \n",
    "\n",
    "More information is [here](https://developer.nvidia.com/blog/gpu-dashboards-in-jupyter-lab/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a757322-fcee-4983-83a8-dbb3fdbf1eaa",
   "metadata": {},
   "source": [
    "![SegmentLocal](images/NVIDIA_GPUDashboard.gif \"segment\")\n",
    "\n",
    "*[Image source](https://developer.nvidia.com/blog/gpu-dashboards-in-jupyter-lab/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1731ac1-bece-4685-bb95-19cb74dff0f8",
   "metadata": {},
   "source": [
    "## [JAX](https://jax.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ce34a-03fe-4f93-a20a-2a389ef475e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0c07a-9f5a-4db5-9b88-ff42b7a95497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380b32f-962f-4a66-bfb8-eea6df56c9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3210104b-8e88-4ad5-941b-c4a58fc97589",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61e0a1-481e-45be-99cc-6885baa6a725",
   "metadata": {},
   "source": [
    "```{admonition} Key Points\n",
    "\n",
    "- Use Numba to write custom code for CUDA GPUs.\n",
    "- Use RAPIDS libraries for move your data science code to GPUs.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea113ee0-9ed3-4b77-9f5f-eca6600f7d22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Further information\n",
    "\n",
    "### Good practises\n",
    "\n",
    "- Test out ideas on CPUs first, before moving to expensive GPUs.\n",
    "- Consider whether the calculation is worth the additional overhead of sending data to and from the GPU.\n",
    "- Minimise data transfers between the host (CPU) and the device (GPU).\n",
    "\n",
    "### Other options\n",
    "\n",
    "- [pycuda](https://documen.tician.de/pycuda/)\n",
    "    - An alternative to Numba for accessing NVIDIA's CUDA GPUs.\n",
    "- Many libraries can use GPUs automatically if they can detect one e.g., [`TensorFlow`](https://www.tensorflow.org/install/gpu) and [`PyTorch`](https://pytorch.org/docs/stable/notes/cuda.html).\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [CuPy - Sean Farley](https://www.youtube.com/watch?v=_AKDqw6li58), PyBay 2019.  \n",
    "- [cuDF - Mark Harris](https://www.youtube.com/watch?v=lV7rtDW94do), PyCon AU 2019.  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ffad78e3b53a26aeabe29bd69865e9fcde2eed64638bf28084d4e5d53534f3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
